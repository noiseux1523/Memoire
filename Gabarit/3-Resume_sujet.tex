% Résumé du mémoire.
%
%   Le résumé est un bref exposé du sujet traité, des objectifs visés,
% des hypothèses émises, des méthodes expérimentales utilisées et de
% l'analyse des résultats obtenus. On y présente également les
% principales conclusions de la recherche ainsi que ses applications
% éventuelles. En général, un résumé ne dépasse pas quatre pages.
%
%   Le résumé doit donner une idée exacte du contenu du mémoire ou de la thèse. Ce ne
% peut pas être une simple énumération des parties du document, car il
% doit faire ressortir l'originalité de la recherche, son aspect
% créatif et sa contribution au développement de la technologie ou à
% l'avancement des connaissances en génie et en sciences appliquées.
% Un résumé ne doit jamais comporter de références ou de figures.

%TOTAL = 4 pages

\chapter*{RÉSUMÉ}\thispagestyle{headings}
\addcontentsline{toc}{compteur}{RÉSUMÉ}

\setlength{\parindent}{5ex} \ac{TD} are temporary solutions, or workarounds, introduced in portions of software systems in order to fix a problem rapidly at the expense of quality. Such practices are widespread for various reasons: rapidity of implementation, initial conception of components, lack of system's knowledge, developer inexperience or deadline pressure. Even though technical debts can be useful on a short term basis, they can be excessively damaging and time consuming in the long run. Indeed, the time required to fix problems and design code is frequently not compatible with the development life cycle of a project. This is why the issue has been tackled in various studies, specifically in the aim of detecting these harmful debts. \par

One recent and popular approach is to identify technical debts which are self-admitted. The particularity of these debts, in comparison to \ac{TD}, is that they are explicitly documented with comments and intentionally introduced in the source code. \ac{SATD} are not uncommon in software projects and have already been extensively studied concerning their diffusion, impact on software quality, criticality, evolution and actors. Various detection methods are currently used to identify \ac{SATD} but are still subject to improvement. For example, using keywords (\emph{e.g.: hack, fixme, todo, ugly, etc.}) in comments linking to a technical debt or using \ac{NLP} in addition to machine learners. Therefore, this study investigates to what extent previously self-admitted technical debts can be used to provide recommendations to developers writing new source code. The goal is to be able to suggest when to "self-admit" technical debts or when to improve new code being written. \par

To achieve this goal, a machine learning approach was conceived, named \ac{TEDIOUS}, using various types of method-level input features as independent variables to classify design technical debts using self-admitted technical debts as oracle. The model was trained and assessed on nine open source Java projects which contained previously tagged \ac{SATD}. In other words, our proposed machine learning approach aims to accurately predict technical debts in software projects. \par

\ac{TEDIOUS} works at method-level granularity, in other words, it can detect whether a method contains a design debt or not. It was designed this way because developers are more likely to self-admit technical debt for methods or block. \ac{TD} can be classified in different types: design, requirement, test, code or documentation. Only design debts were considered because they represent the largest fraction and each type would require its own analysis.  \par

\ac{TEDIOUS} is trained with \emph{labeled data}, projects with labeled \ac{SATD}, and tested with \emph{unlabeled data}. The labeled data contain methods tagged as \ac{SATD} which were obtained from nine projects analyzed by another research group using a \ac{NLP} approach and manually validated. Projects are of various sizes (\emph{e.g.:} number of classes, methods, comments, etc.) and contain different proportions of design debts. From the labeled data are extracted various kinds of metrics: source code metrics, readability metrics and warnings raised by static analysis tools. Nine source code metrics were retained to capture the size, coupling, complexity and number of components in methods. The readability metric takes in consideration indentation, line length and identifier lengths just to name a few features. Two static analysis tools are used to check for poor coding practices. \par

Feature preprocessing is applied to remove unnecessary features and keep the ones most relevant to the dependent variable. Some features are strongly correlated between each others and keeping all of them is redundant. Other features undergo important or no variations in our dataset, they would not be useful to build a predictor and thus are removed as well. Additionally, to achieve good cross-project predictions, metrics are normalized because the source code of different projects can differ in terms of size and complexity. Finally, the dataset is unbalanced, which means the amount of methods labeled as \ac{SATD} is small. Over-sampling was applied on the minority class to generate artificial instances from the existing ones. \par

Machine learnings models are built based on the training set and predictions are evaluated from the test set. Five kinds of machine learners were tested: Decision Trees (J48), Bayesian classifiers, Random Forests, Random Trees and Bagging with Decision Trees. These models were retained to gather a wide variety of results, from different algorithms which were considered the most appropriate and accurate for the context of this study. \par

Globally, the goal of this study is to assess the \ac{SATD} prediction performance of our approach. The quality focus is understandability and maintainability of the source code, achieved by tracking existing \ac{TD}. The perspective is to be able to suggest when to admit those \ac{TD}. Three research questions are aimed to be addressed: 

\begin{itemize}
\item \textbf{RQ1}: How does \ac{TEDIOUS} work for recommending \ac{SATD} with-project?
\item \textbf{RQ2}: How does \ac{TEDIOUS} work for recommending \ac{SATD} across-project?
\item \textbf{RQ3}: How would a method-level smell detector compare with \ac{TEDIOUS}?
\end{itemize}

To address \textbf{RQ1}, 10-fold cross validation was performed on all projects, which means a machine learner is trained with 90\% of a project's methods and tested with 10\% of them. The process is repeated 10 times to reduce the effect of randomness. A similar approach is used for \textbf{RQ2}, a machine learner is trained with 8 projects and is tested with 1 project. \par

To assess the performance of \ac{TEDIOUS}, standard metrics such as precision, recall and F1 score are computed for the \ac{SATD} category. These metrics are based on the amount of \ac{TP}, \ac{FP} and \ac{FN}. To complement the evaluation, accuracy, \ac{MCC} and \ac{ROC} \ac{AUC} are computed, partly to take into account the amount of \ac{TN}. What is aimed for in a machine learning model performance is a balance between precision and recall, to suggest as many \emph{correct} \ac{TD} to admit as possible. \ac{MCC} and \ac{AUC} are useful indicators to reduce the effect of chance. The importance of feature metrics is also taken into account to evaluate the models. \par

To address \textbf{RQ3}, the performance of a smell detector, \ac{DECOR}, was computed and evaluated in classifying as \ac{TD} methods labeled as \ac{SATD}. Only method-level smells were analyzed, similarly to \ac{TEDIOUS}. Finally, some \ac{FP} and \ac{FN} were qualitatively discussed in order to explain the limits of our approach. \par

For \textbf{RQ1}, results showed that Random Forest classifiers achieved the best performance recommending design debts. The average precision obtained is 49.97\% and the recall 52.19\%. The MCC and AUC values of each project generally indicated healthy classifiers. Balancing the dataset increased recall at the expense of precision and code readability, complexity and size played a significant role in building the predictors. \par

For \textbf{RQ2}, cross-project prediction increased the performance of predictors compared to the standard cross-validation on singular projects because of a larger and more diverse training set. The average precision obtained is 67.22\% and the recall 54.89\%. The MCC and AUC values still indicated healthy classifiers. Similarly to within project predictions, code readability, size and complexity played the most important role in recommending when to self-admit design \ac{TD}. \par

For \textbf{RQ3}, \ac{LM} and \ac{LP} were the specific smells targeted and evaluated by \ac{DECOR}, similar to \ac{LOC} and number of parameters metrics, which played an important role in training machine learners in the context of our study. However, the detectors of \ac{DECOR} were unable to achieve similar performance as \ac{TEDIOUS}. The \emph{F$_{1}$} score for the union of \ac{LM} and \ac{LP} couldn't surpass 22\% and the \ac{MCC} value leaned towards a low prediction correlation. \par

As for the qualitative analysis of correctly classified or misclassified \ac{SATD}, several observations were made. When analyzing \ac{TP}, \ac{TEDIOUS} was able to correctly identify a wide variety of methods labeled as \ac{SATD} even though their defining features were significantly different. \ac{FP} targeted intrinsically complex and long methods that weren't initially labeled as \ac{SATD}, which isn't necessarily bad because it may lead the developer to review these pieces of code. As an example of \ac{FN}, some comments mention the presence of a problem in a block of code which isn't trivial when manually analyzed, or when using \ac{TEDIOUS}. \par

Some threats can affect the validity of our study. For \emph{construct validity} threats, the measurement errors of labeled design \ac{SATD} and metrics represent an issue. For \emph{internal validity} threats, default parameters only were applied for the machine learners. Some kind of optimization could be applied obtain better machine learner configurations. The proper use of performance diagnostics (\ac{AUC}, \ac{MCC}) allowed us to reduce the \emph{conclusion validity} threats. For \emph{reliability validity} threats, all necessary details are provided to replicate our study. For \emph{external validity} threats, it cannot be guaranteed that our results can be generalized to all Java projects considering the small amount of projects analyzed. Our approach would need to be extended to more projects, domains or programming languages. \par

This paper describes \ac{TEDIOUS}, a method-level machine learning approach designed to recommend when a developer should self-admit a design technical debt based on size, complexity, readability metrics, and static analysis tools checks. Within-project performance values based on 9 open source Java projects lead to promising results: about 50\% precision, 52\% recall and 93\% accuracy. Cross-project performance was even more promising: about 67\% precision, 55\% recall and 92\% accuracy. Highly unbalanced data represented the biggest issue in obtaining higher performance values. For bigger projects, precision and recall above 88\% were obtained. \par

Different applications could be made of \ac{TEDIOUS}. It could be used as a recommendation system for developers to know when to document \ac{TD} they introduced. Secondly, it could help customize warnings raised by static analysis tools, by learning from previously defined \ac{SATD}. Thirdly, it could compliment existing smell detectors to improve their performance, like \ac{DECOR}. As for our future work, a larger dataset will be studied to see if adding more information could be beneficial to our approach. Additionally, we plan to extend \ac{TEDIOUS} to the recommendation of more types of technical debts.





















