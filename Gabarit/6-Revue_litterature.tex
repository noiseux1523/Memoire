\Chapter{LITERATURE REVIEW}\label{sec:RevLitt}

% TOTAL = 4 pages

\setlength{\parindent}{5ex}The subject of this study can be divided in four relevant topics, which will be summarized in this chapter. The literature review introduces each topic based on previous studies and research papers. The first one addresses the relationship between technical debt and source code metrics. The second defines the nature of self-admitted technical debts. The last topic combines the code smell detection approaches and automated static analysis tools.

\section{Relationship Between Technical Debt and Source Code Metrics}

Many researchers have tried to relate technical debts, more specifically design and code, to source code metrics in order to detect them. One of the approach is based on a technique for detecting design flaws, built on top of a set of metric rules capturing coupling, cohesion and encapsulation \citep{marinescu2012assessing}. Another study empirically validated the relationship between \ac{TD} and software quality models. Three \ac{TD} detection methods were compared with \ac{QMOOD} \citep{bansiya2002hierarchical} and only one of them had a strong correlation to quality attributes reusability and understandability \citep{griffith2014correspondence}. Another team studied how five different tools detect technical debts, their principal features, differences and missing aspects \citep{fontana2016technical}. They focused on the impact of design smells on code debt to give advices on which design debt should be prioritized for refactoring. These tools all take into account metrics, smells, coding rules and architecture violations but there is only a limited agreement among them and they still ignore some important pieces of information.

\section{Self-Admitted Technical Debt}

Many studies have been conducted in order to describe and classify the nature of self-admitted technical debts. \citet{PotdarS14} investigated technical debts in the source code of open source projects and they found out that developers frequently self-admit \ac{TD} they introduce, explaining why this particular block of code is temporary and needs to be reworked in the form of comments. They are some of the first to acknowledge the existence of \ac{SATD} and to propose a detection method using pattern matching in source code comments. \citet{MaldonadoS15} analyzed developers' comments in order to examine and quantify the different types of \ac{SATD}. A similar approach to \citet{PotdarS14} is followed, using pattern matching, to classify the \ac{SATD} into five types: design, defect, documentation, requirement and test. It was found that design debts are the most common, making up between 42\% and 84\% of all comments. \par

\citet{BavotaR16} performed a large-scale empirical study on self-admitted technical debt in open source projects. They studied its diffusion and evolution, the actors involved in managing \ac{SATD} and the relationship between \ac{SATD} and software quality. They showed that there is on average 51 instances of \ac{SATD} per system, that code debts are the most frequent, followed by defect and requirement debts, that the number of instances increases over time because they are not fixed by developers, and that they normally survive for a long time. Like \citet{griffith2014correspondence}, they found no real correlation between \ac{SATD} and quality metrics (\ac{WMC}, \ac{CBO}, Buse and Weimer readability). \par

\citet{wehaibi2016examining} also studied the relation between self-admitted technical debt and software quality. Their approach is based on investigating if more defects are present in files with more \ac{SATD}, if \ac{SATD} changes are more likely to cause the emergence of future defects and whether they are more difficult to perform. They found that no real trend was noticed between \ac{SATD} and defects, \ac{SATD} changes did not introduce more future defects compared to none \ac{SATD} changes but they are indeed more difficult to perform. \par

A new approach based on \ac{NLP} techniques was used recently to detect self-admitted technical debts, more specifically design and requirement debts \citep{MaldonadoNLP}. They extracted comments from ten open source projects, cleaned them to remove the ones considered irrelevant and manually classified them into the different types of \ac{SATD}. This dataset was then used as the training set for a maximum entropy classifier. It turned out that the model could accurately identify \ac{SATD} and outperform the pattern matching method of \citet{PotdarS14}. Comments mentioning sloppy or mediocre source code were the best indicators of design debts and comments related to partially implemented requirement were the best for requirement debts. \par

The detection of self-admitted technical debts is a major research approach in the study of \ac{SATD}, however, this is not the purpose of our work. We do not propose a new approach using source code comments information, instead, we gather information about the structure of the code at method-level in order to recommend to developers when to self-admit technical debts. \par

\section{Code Smell Detection and Automated Static Analysis Tools}

Several approaches to detect code smells have been proposed in today's literature. Reading techniques have been created to guide developers in identifying \ac{OO} designs \citep{Travassos99-ACM-Inspections}. Some formulate metrics-based rules as a detection strategy that can capture poor design practices \citep{Marinescu04-ICSM-DetectionStrategies} or use these software metrics to characterize bad smells \citep{Munro05-BadSmellIdentification}. Others such as \citet{moha2010decor} propose \ac{DECOR}, an approach using rules and thresholds on various metrics. Many detection techniques rely on structural information, however, \citet{PalombaBPOPL15} exploited change history information to propose \ac{HIST} to detect instances of five different code smells, with promising results. \citet{FokaefsTSC11} used graph matching to propose JDeodorant, an Eclipse plugin that automatically applies refactoring on "God Classes". Using graph matching also, a methodology recommending "Move Method" refactoring opportunities for "Feature Envy" bad smells was also proposed to reduce coupling and increase cohesion \citep{Tsantalis:tse2009}. Lastly, machine learning techniques are also popular. \citet{fontana2016comparing} compared and experimented 16 different machine learning algorithms to detect code smells, \citet{khomh2009bayesian} proposed a Bayesian approach to detect code and design smells and \citet{maiga2012support} proposed SVMDetect, a new approach to detect anti-patterns using a \ac{SVM} technique. \par 

Our approach is different from these previous ones for two main reasons. Firstly, they use structural or historical information and metrics from the code to detect smells. In addition to these characteristics, we use feedback provided by developers, in the form of \ac{SATD} comments which leads to the identification of technical debts. Secondly, we also use warnings generated by \ac{ASAT} to portray an even better representation of the source code quality. \par 

The subject of automated static analysis tools have already been widely covered to analyze its benefits on the development of software projects. The correlation and correspondence between post-release defects and warnings issued by the bug finding tool FindBugs was studied to understand the actual gains provided by \ac{ASAT} \citep{couto2013static}. Only a moderate correlation was found and no correspondence between defects and raised warnings. On the other hand, three \ac{ASAT} were evaluated by \citet{wedyan2009effectiveness} showing that they could successfully recommend refactoring opportunities to developers. FindBugs static analysis tool was also evaluated by \citet{ayewah2007evaluating} to quantify its accuracy and the value of warnings raised. They found that warnings were mostly considered relevant by developers and that they were willing to make the appropriate modifications to fix the issues. \citet{BellerBMZ16} performed a large-scale evaluation of \ac{ASAT} in \ac{OSS}. They found that the use of \ac{ASAT} is widespread but no strict usage policy is imposed in software projects. Generally, the automated static analysis tools are used with their default configuration, only a small amount is significantly changed. Also, \ac{ASAT} configurations experience little to no modifications over time. \par 

Many of the mentioned studies share common views and purposes with our study and \ac{TEDIOUS}. However, as far as we know, our work stands out because \ac{TEDIOUS} is the first approach that attempts to predict technical debts at method-level with a wide variety of easy to use and extract information.













