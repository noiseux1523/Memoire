\Chapter{LITERATURE REVIEW}\label{sec:RevLitt}

% TOTAL = 4 pages

\setlength{\parindent}{5ex}The literature related to this thesis can be divided in four topics, which will be summarized in this chapter. The first one addresses the relationship between technical debt and source code metrics. The second defines what are self-admitted technical debts. The third topic describes code smell detection approaches and the last one covers the usage of automated static analysis tools.

\section{Relationship Between Technical Debt and Source Code Metrics}

Many researchers have tried to link technical debts, more specifically design and code types, to source code metrics. \citet{marinescu2012assessing} proposed an approach based on a technique for detecting design flaws and built on top of a set of metric rules capturing coupling, cohesion and encapsulation. \citet{griffith2014correspondence} empirically validated the relationship between \ac{TD} and software quality models. Three \ac{TD} detection methods were compared with \ac{QMOOD} \citep{bansiya2002hierarchical} and only one of them had a strong correlation to quality attributes, namely reusability and understandability. A larger study was performed by \citet{fontana2016technical} where they analyzed how five different tools detect technical debts, their principal features, differences and missing aspects. They focused on the impact of design smells on code debt to give advices on which design debt should be prioritized for refactoring. These tools all took into account metrics, smells, coding rules and architecture violations. However, there was only a limited agreement among tools and they still ignored some important pieces of information.

\section{Self-Admitted Technical Debt}

Many studies have been conducted in order to describe and classify the nature of self-admitted technical debts. \citet{PotdarS14} investigated technical debts in the source code of open source projects and they found out that developers frequently self-admit \ac{TD} they introduce, explaining in the form of comments why these particular blocks of code are temporary and need to be reworked. They are some of the first to acknowledge the existence of \ac{SATD} and to propose a detection method using pattern matching in source code comments. \citet{MaldonadoS15} analyzed developers' comments in order to define and quantify different types of \ac{SATD}. An approach similar to the one of \citet{PotdarS14} is followed, which is using pattern matching to classify \ac{SATD} into five types: design, defect, documentation, requirement and test. It was found that design debts are the most common, making up between 42\% and 84\% of all comments in software projects. \par

\citet{BavotaR16} performed a large-scale empirical study on self-admitted technical debts in open source projects. They studied their diffusion and evolution, the actors involved in managing \ac{SATD} and the relationship between \ac{SATD} and software quality. They showed that there is on average 51 instances of \ac{SATD} per system, that code debts are the most frequent, followed by defect and requirement debts, that the number of instances increases over time because they are not fixed by developers, and that they normally survive for a long time. Like \citet{griffith2014correspondence}, they found no real correlation between \ac{SATD} and quality metrics (\ac{WMC}, \ac{CBO}, Buse and Weimer readability). \par

\citet{wehaibi2016examining} also studied the relation between self-admitted technical debts and software quality. Their approach is based on investigating if more defects are present in files with more \ac{SATD}, if \ac{SATD} changes are more likely to cause the emergence of future defects and whether changes are more difficult to perform or not. They found that no real trend could be made between \ac{SATD} and defects, that \ac{SATD} changes did not introduce more future defects than no-\ac{SATD} changes but that they are indeed more difficult to perform. \par

\citet{MaldonadoNLP} recently proposed a new approach based on \ac{NLP} techniques to detect self-admitted technical debts, more specifically design and requirement debts. They extracted comments from ten open source projects, cleaned them to remove irrelevant ones and manually classified them into the different types of \ac{SATD}. This dataset was then used as the training set for a maximum entropy classifier. It turned out that the model could accurately identify \ac{SATD} and outperform the pattern matching method of \citet{PotdarS14}. Comments mentioning sloppy or mediocre source code were the best indicators of design debts and comments related to partially implemented requirement were the best for requirement debts. \par

Contrary to previous studies, the goal of TEDIOUS is to detect methods that are TD prone. This is to say differently from \citet{MaldonadoS15}, our goal is not to classify comments but rather to categorize methods based on those classified comments.

\section{Code Smell Detection}

Several approaches to detect code smells have been proposed in today's literature: \citet{Travassos99-ACM-Inspections} developed reading techniques to guide developers in identifying defects in \ac{OO} designs, \citet{Marinescu04-ICSM-DetectionStrategies} formulated metrics-based rules as a detection strategy that can capture poor design practices and \citet{Munro05-BadSmellIdentification} used software metrics to characterize bad smells. Others such as \citet{moha2010decor} proposed \ac{DECOR}, an approach using rules and thresholds on various metrics to detect smells. This smell detector is in fact used to compare its performance with TEDIOUS. 

Many detection techniques rely on structural information, however, \citet{PalombaBPOPL15} exploited change history information to propose \ac{HIST}, a smell detector that identifies instances of five different code smells, with promising results. On the other hand, \citet{FokaefsTSC11} used graph matching to propose JDeodorant, an Eclipse plugin that automatically applies refactoring on "God Classes". Using graph matching also, \citet{Tsantalis:tse2009} proposed a methodology recommending "Move Method" refactoring opportunities for "Feature Envy" bad smells to reduce coupling and increase cohesion. 

Machine learning techniques are also popular. \citet{fontana2016comparing} compared and experimented with 16 different machine learning algorithms to detect code smells, \citet{khomh2009bayesian} proposed a Bayesian approach to detect code and design smells, and \citet{maiga2012support} proposed SVMDetect, a new approach to detect anti-patterns using a \ac{SVM} technique. \par 

TEDIOUS is different from these previous approaches for two main reasons. Firstly, they use structural or historical information and metrics from the code to detect smells. However, in addition to these characteristics, we also use feedback provided by developers, in the form of \ac{SATD} comments which leads to the identification of technical debts. Secondly, we also use warnings generated by \ac{ASAT} to portray an even better representation of the source code quality. \par 

\section{Automated Static Analysis Tools}

The subject of automated static analysis tools have already been widely covered to analyze its benefits on the development of software projects. To understand the actual gains provided by automated static analysis tools, \citet{couto2013static} studied the correlation and correspondence between post-release defects and warnings issued by the bug finding tool FindBugs. Only a moderate correlation and no correspondence were found between defects and raised warnings. On the other hand, three \ac{ASAT} were evaluated by \citet{wedyan2009effectiveness} showing that they could successfully recommend refactoring opportunities to developers. \citet{ayewah2007evaluating} also evaluated an FindBugs, its performance was measured to quantify its accuracy and the value of warnings raised. They found that warnings were mostly considered relevant by developers and that they were willing to make the appropriate modifications to fix the issues. \citet{BellerBMZ16} performed an evaluation of several ASAT on an even larger scale. They found that the use of \ac{ASAT} is widespread but no strict usage policy is imposed in software projects. Generally, the automated static analysis tools are used with their default configuration, only a small amount is significantly changed. Also, \ac{ASAT} configurations experience little to no modifications over time. \par 

Many of the mentioned studies share common views and purposes with our research and \ac{TEDIOUS}. However, as far as we know, TEDIOUS stands out because it is the first approach that attempts to predict technical debts at method-level with a wide variety of easy to use and to extract information.













