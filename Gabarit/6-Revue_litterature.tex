\Chapter{LITERATURE REVIEW}\label{sec:RevLitt}

% TOTAL = 4 pages

\setlength{\parindent}{5ex}The literature related to this thesis can be divided in four topics, which will be summarized in this chapter. The first one addresses the relationship between technical debt and source code metrics. The second defines what are self-admitted technical debts. The third topic describes code smell detection approaches. The last one covers the usage of automated static analysis tools.

\section{Relationship Between Technical Debt and Source Code Metrics}

Many researchers have tried to link technical debts, more specifically design and code, to source code metrics. \citet{marinescu2012assessing} proposed an approach based on a technique for detecting design flaws, built on top of a set of metric rules capturing coupling, cohesion and encapsulation. \citet{griffith2014correspondence} empirically validated the relationship between \ac{TD} and software quality models. Three \ac{TD} detection methods were compared with \ac{QMOOD} \citep{bansiya2002hierarchical} and only one of them had a strong correlation to quality attributes reusability and understandability. A larger study was performed by \citet{fontana2016technical} where they analyzed how five different tools detect technical debts, their principal features, differences and missing aspects. They focused on the impact of design smells on code debt to give advices on which design debt should be prioritized for refactoring. These tools all took into account metrics, smells, coding rules and architecture violations. However, there was only a limited agreement among them and they still ignored some important pieces of information.

\section{Self-Admitted Technical Debt}

Many studies have been conducted in order to describe and classify the nature of self-admitted technical debts. \citet{PotdarS14} investigated technical debts in the source code of open source projects and they found out that developers frequently self-admit \ac{TD} they introduce, explaining why this particular block of code is temporary and needs to be reworked in the form of comments. They are some of the first to acknowledge the existence of \ac{SATD} and to propose a detection method using pattern matching in source code comments. \citet{MaldonadoS15} analyzed developers' comments in order to examine and quantify the different types of \ac{SATD}. A similar approach to \citet{PotdarS14} is followed, using pattern matching, to classify the \ac{SATD} into five types: design, defect, documentation, requirement and test. It was found that design debts are the most common, making up between 42\% and 84\% of all comments. \par

\citet{BavotaR16} performed a large-scale empirical study on self-admitted technical debt in open source projects. They studied its diffusion and evolution, the actors involved in managing \ac{SATD} and the relationship between \ac{SATD} and software quality. They showed that there is on average 51 instances of \ac{SATD} per system, that code debts are the most frequent, followed by defect and requirement debts, that the number of instances increases over time because they are not fixed by developers, and that they normally survive for a long time. Like \citet{griffith2014correspondence}, they found no real correlation between \ac{SATD} and quality metrics (\ac{WMC}, \ac{CBO}, Buse and Weimer readability). \par

\citet{wehaibi2016examining} also studied the relation between self-admitted technical debt and software quality. Their approach is based on investigating if more defects are present in files with more \ac{SATD}, if \ac{SATD} changes are more likely to cause the emergence of future defects and whether they are more difficult to perform. They found that no real trend was noticed between \ac{SATD} and defects, \ac{SATD} changes did not introduce more future defects compared to none \ac{SATD} changes but they are indeed more difficult to perform. \par

\citet{MaldonadoNLP} recently proposed new approach based on \ac{NLP} techniques to detect self-admitted technical debts, more specifically design and requirement debts. They extracted comments from ten open source projects, cleaned them to remove the ones considered irrelevant and manually classified them into the different types of \ac{SATD}. This dataset was then used as the training set for a maximum entropy classifier. It turned out that the model could accurately identify \ac{SATD} and outperform the pattern matching method of \citet{PotdarS14}. Comments mentioning sloppy or mediocre source code were the best indicators of design debts and comments related to partially implemented requirement were the best for requirement debts. \par

Contrary to previous studies, the goal of TEDIOUS is to detect methods that are TD prone. This is to say differently from \citet{MaldonadoS15}, our goal is not to classify comments but rather to categorize methods based on those classified comments.

\section{Code Smell Detection}

Several approaches to detect code smells have been proposed in today's literature: \citet{Travassos99-ACM-Inspections} developed reading techniques to guide developers in identifying \ac{OO} designs, \citet{Marinescu04-ICSM-DetectionStrategies} formulated metrics-based rules as a detection strategy that can capture poor design practices and \citet{Munro05-BadSmellIdentification} used software metrics to characterize bad smells. Others such as \citet{moha2010decor} proposed \ac{DECOR}, an approach using rules and thresholds on various metrics for smell detection, which will be used to compare with TEDIOUS. 

Many detection techniques rely on structural information, however, \citet{PalombaBPOPL15} exploited change history information to propose \ac{HIST}, a smell detector that identifies instances of five different code smells, with promising results. On the other hand, \citet{FokaefsTSC11} used graph matching to propose JDeodorant, an Eclipse plugin that automatically applies refactoring on "God Classes". Using graph matching also, \citet{Tsantalis:tse2009} proposed a methodology recommending "Move Method" refactoring opportunities for "Feature Envy" bad smells to reduce coupling and increase cohesion. 

Machine learning techniques are also popular. \citet{fontana2016comparing} compared and experimented 16 different machine learning algorithms to detect code smells, \citet{khomh2009bayesian} proposed a Bayesian approach to detect code and design smells, and \citet{maiga2012support} proposed SVMDetect, a new approach to detect anti-patterns using a \ac{SVM} technique. \par 

TEDIOUS is different from these previous approaches for two main reasons. Firstly, they use structural or historical information, and metrics from the code to detect smells. However, in addition to these characteristics, we also use feedback provided by developers, in the form of \ac{SATD} comments which leads to the identification of technical debts. Secondly, we also use warnings generated by \ac{ASAT} to portray an even better representation of the source code quality. \par 

\section{Automated Static Analysis Tools}

The subject of automated static analysis tools have already been widely covered to analyze its benefits on the development of software projects. To understand the actual gains provided by automated static analysis tools, \citet{couto2013static} studied the correlation and correspondence between post-release defects and warnings issued by the bug finding tool FindBugs. Only a moderate correlation and no correspondence were found between defects and raised warnings. On the other hand, three \ac{ASAT} were evaluated by \citet{wedyan2009effectiveness} showing that they could successfully recommend refactoring opportunities to developers. \citet{ayewah2007evaluating} also evaluated an ASAT, the performance of FindBugs static analysis tool was measured to quantify its accuracy and the value of warnings raised. They found that warnings were mostly considered relevant by developers and that they were willing to make the appropriate modifications to fix the issues. \citet{BellerBMZ16} performed an evaluation of several ASAT on an even larger scale. They found that the use of \ac{ASAT} is widespread but no strict usage policy is imposed in software projects. Generally, the automated static analysis tools are used with their default configuration, only a small amount is significantly changed. Also, \ac{ASAT} configurations experience little to no modifications over time. \par 

Many of the mentioned studies share common views and purposes with our research and \ac{TEDIOUS}. However, as far as we know, TEDIOUS stands out because it is the first approach that attempts to predict technical debts at method-level with a wide variety of easy to use and to extract information.













