\Chapter{THE APPROACH AND STUDY DEFINITION}\label{sec:Theme1}

%TOTAL = 15 pages

\section{The Approach}

%2 pages

This section will describe the steps followed to design \ac{TEDIOUS}, our proposed machine learning detector to identify design technical debts to self-admit. It will also define its characteristics, how it works and how to use it. \ac{TEDIOUS} works at method-level since it is typically the granularity at which developers introduce \ac{SATD} \citep{MaldonadoS15,PotdarS14}. In other terms, it is able to detect whether a method contains a design technical debt or not. Class-level granularity would be to coarse because technical debts normally admitted by developers are related to specific and punctual issues in the source code. Additionally, a class could contain a \ac{TD} but it would be impossible to precisely identify the source of the problem since a class contains several methods and \ac{LOC}. \par 

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/approach.pdf}
	\caption{Proposed approach for recommending SATD with TEDIOUS.}
	\label{fig:approach}
	\vspace{-4mm}
\end{figure}

\ac{TEDIOUS} works as shown in Figure \ref{fig:approach}. Two datasets are required as inputs: the training set and the test set. The training set contains labeled data, which is source code from a project where technical debts are known and have been self-admitted through comments. The test set contains unlabeled data, which can be any source code under development or already released where \ac{TEDIOUS} can attempt to recommend where \ac{TD} should be self-admitted or where source code should be improved. \par 
	
For the training set, various kinds of metrics and static analysis warnings are extracted from the source code as well as \ac{SATD} methods in order to build an oracle to train the model. These labeled \ac{SATD} methods are essential for the machine learner since supervised learning is performed, meaning each method is labeled as true (containing a TD) or false. \par 

Once all the information is extracted, feature preprocessing and selection is applied. Multi-collinearity, a phenomenon occurring when two predictor variables are highly correlated, meaning that one can be linearly predicted by the other, is dealt with. Feature selection is applied to retain only the most relevant variables to train the predictor. Finally, re-balancing is performed to address the issue of the low amount of positive examples, \textit{i.e.} \ac{SATD} methods. With the preprocessed features and the oracle now defined (each method is labeled as \ac{SATD} or not), the machine learners can be trained. \par 

In parallel, the test set is also being prepared. The same features are extracted from the source code but no \ac{SATD} matching is required since the data is unlabeled. \ac{SATD} are only required for the oracle, which is used for training the models. A similar feature filtering is applied, except for the re-balancing since it is only required on the labeled data. With both the test set and the previously trained classifier, predictions can be made on the test set in order to recommend when to self-admit technical debts. \par

Each step of the process will be described in the following sections. Firstly, the features used in more details: source code metrics and warnings raised by static analysis tools. Secondly, the method employed to identify \ac{SATD}. Thirdly, the feature preprocessing: multi-collinearity, feature selection and re-balancing. Finally, the training and application of the machine learning models.

\subsection{Features}

%3 pages

Three pieces of information extracted from the source code are necessary to accurately describe the source code: structural metrics, readability metrics and warnings raised by static analysis tools. There are reasons these specific characteristics were retained to describe the source code. The structural metrics are essential to capture symptoms of complex, heavily couples and poorly designed code. These metrics explain the quality of the implementation and the software's design. The readability metrics quantify symptoms of poorly documented code or hard to read and understand. Warnings from static analysis tools are related to more specific bad choices rather than globally bad code. They are issues which could lead to low maintainability and understandability of the code or which could potentially introduce defects in the future. In the following sections, these metrics and warnings are described more in depth.

\subsubsection{Source Code Metrics}

Source code metrics are extracted to characterize its size, coupling, complexity and number of comments for readability. Nine source code metrics are used. To define the size, metrics like \ac{LOC} or number of statements are calculated. For coupling, a metric such as number of call sites is computed. For complexity, McCabe cyclomatic complexity \citep{mccabe1990reverse}, number of defined variables, number of expressions and number of identifiers are calculated. It is important to know that not all comments are considered. \ac{SATD} related comments are ignored in the empirical evaluation to avoid \ac{TEDIOUS} becoming a self-prophecy. This issue will be covered later in the study design. \par

Firstly, to extract those source code metrics, an XML representation of the Java source code was generated using the srcML tool \citep{CollardKM03} and metrics' computation was applied to it. It was also required to link comments to their related methods. The rule used was that any comments directly preceding or inside a method was assigned to it, rather it be a block of comments or just a single line. This step is essential to be able to classify which method is a \ac{SATD} and which one is not. Some methods were excluded from our analysis because they did not fit the context of our research, namely getter and setter methods since they are irrelevant with design debts. To do so, we looked for method prefixes matching \emph{get} or \emph{set} and made up from at least a single line of code. When these two criteria were covered, the method was removed from the dataset. \par 

%Using  the {\em  srcML} tool, we processed each Java file, and stored the XML Java representation. We then used the Python ElementTree XML package to build the file level document object model; visiting the DOM and extracting methods along with  the attached comments and computing a set of structural metrics \Foutse{are these metrics different from those described in the approach section?}. We assigned to a method the method proceeding comments not separated from the method definition by any other statement. Then, as also described in Section \ref{sec:approach}, for each method we extracted source code structural metrics, the Buse and Weimer readability metric, and then we executed CheckStyle and PMD on each source code file. Finally, we mapped CheckStyle and PMD warnings onto methods, by knowing the line(s) in which a warning occurred and the line span of each method.

Secondly, to extract the code readability metric, we use the one proposed by \citet{Buse:tse2010} and its related tool, based on a machine learning approach. This metric is based on specific characteristics of the source code: indentation, line length, identifier length, comment density, and the use of keywords and operators. The tool was also designed using feedback from human annotators, they were asked to rate the readability of code snippets, that was then used to classify snippets as "less" or "more" readable. A value between 0 and 1 is then computed based on these characteristics, 1 being the highest readability. The readability metric was considered relevant because, as mentioned by \citet{BavotaR16}, code readability is strongly correlated with the introduction of technical debts. Source code that is difficult to read is consequently difficult to maintain and understand \citep{Buse:tse2010} thus more likely to contain \ac{TD}. Finally, here are the nine source-code metrics we extracted: \par

\begin{itemize}
\item \textit{LOC}: Number of lines of code in the body of a method. % (excluding comments).
\item \textit{Number of statements}: Number of occurrences of expression statements in a method. In case of local class definitions, the number of statements in the enclosing method is increased by the total number of statements of the local class.
\item \textit{Number of comments}: Number of single-line and multi-line comments in a method.
\item \textit{McCabe cyclomatic complexity}: Number of linearly independent paths of a method \citep{mccabe90}.
\item \textit{Number of passed parameters}: Number of parameters of a method.
\item \textit{Number of identifiers}: Number of unique identifiers in a method.
\item \textit{Number of call sites}: Number of locations in the code where zero or more arguments are passed to a method, or zero or more return values are received from the method.
\item \textit{Number of declarations}: Number of variable and class declaration in a method.
\item \textit{Number of expressions}: Number of expressions contained in a method.
\end{itemize}

\subsubsection{Warnings Raised by Static Analysis Tools}

Warnings raised by static analysis tools are essential to detect poor coding practices, which are also related to the introduction of technical debts. We cannot directly relate a single flagged practice to a \ac{TD}. However, we can wonder if several flags are justifiable or not, and if they can be caused by the presence of a \ac{TD} in the source code. Having this hypothesis in mind, we used two popular static analysis tools, namely CheckStyle and PMD. Firstly, CheckStyle \citep{checkstyle} is widely used to check the adherence to coding standards and pieces of code that are potentially smells. It performs its analysis based on a configuration file, which can be modified at the discretion of the users. For our research, the default configuration containing code styles defined by Oracle and 43 checks was used. PMD \citep{pmd} main goal is to find common programming flaws such as unused objects, unnecessary catch blocks or incomprehensible naming. Similarly to CheckStyle, the default configuration was used, which contains 168 checks. Several reasons explains the choice of these two static analysis tools: they are adopted by \ac{OSS} \citep{BellerBMZ16}, they provide a wide range of warnings related to code styles and programming practices and they can be executed on source code statically. It is important to know that \ac{SATD} comments were removed when performing the analysis.

\subsection{Identification of Self-Admitted Technical Debt}

%0.5 page

As mentioned previously, the purpose of this thesis is not to propose a novel approach to detect \ac{SATD} using information from comments. Previous work has been completed aiming to address this issue by using  matching \citep{MaldonadoS15}, \citep{PotdarS14} or \ac{NLP} combined with machine learners \citep{maldonado17}. However, we still needed a dataset with methods tagged as design debt to train our machine learning models. \citet{maldonado17}, which worked on the \ac{NLP} approach, published a dataset of 10 open source projects annotated with methods tagged as technical debt or not. We used this dataset for our machine learning models and only the design debts were retained. \par

Some preprocessing had to be done on the dataset since it reported \ac{SATD} at file-level and not method-level. To link \ac{SATD} to methods, we matched the \ac{SATD} comment string to the comments attached to methods. We used pattern matching in order to achieve this result, making sure that method comment strings completely matched \ac{SATD} comments before tagging it as a technical debt.

\subsection{Feature Preprocessing}

%3 pages

Several characteristics have been extracted from the source code, however, not all of them are relevant or necessary to train our models. Some clean up have to be done to reduce the size of the data input and improve the training phase. Multi-collinearity have to be dealt with, feature selection as to be applied as well as training set re-balancing.

\subsubsection{Multi-Collinearity}

We computed several features to characterize the source code of the projects, however, some of them can be strongly correlated and vary in the same way. Keeping these pairs of features would cause redundancy since they can be mutually linearly predicted. That is why, when facing a pair of such features, we only keep the one that better correlates with the dependent variable, the \ac{SATD} methods. The \emph{varclus} function in R, from the \emph{Hmisc} package, was used to help us achieve this preprocessing. This function performs a hierarchical cluster analysis of features to detect when two variable are positive based on similarity measures. It is mainly used to assess collinearity and redundancy, consequently resulting in data reduction. Hoeffding D statistic, squared Pearson or Spearman correlations measures can be applied. In this research, the Spearman's $\rho$ rank correlation measure was retained. To identify the problematic duos, the cluster tree has to be cut at a particular level of $\rho^2$, which represents the correlation value. In our case, a value of $\rho^2=0.64$ was used since it corresponds to a strong correlation \citep{Cohen-1988}.

\subsubsection{Feature Selection and Normalization}

Some features will vary a lot or not at all between methods. These features are not useful to build a predictor because they can't be used as proper learning variables, they won't be determinant compared to all the other features. The process of selecting only a relevant subset of all possible features is called feature selection. Several reasons can justify going through this selection: improving the prediction performance of machine learning models, improve the speed and cost-effectiveness of predictors, and simplify the models to better understand and interpret the underlying process behind the dataset generation \citep{guyon2003introduction}. \par 

The \emph{RemoveUseless} filter implemented in Weka \citep{hall2009weka} was used to perform feature selection. It looks for features that never vary or features that vary too much. For the latter, it looked for features that had a percentage of variance above a specific treshold, which was set to 99\%. \par 

In addition to feature selection, the metrics were also normalized. Several projects were analyzed, all having significant differences in complexity and size characteristics. Those aspects of the dataset will be covered in the study definition section. This normalization is necessary to reduce the effect of those differences on the predictor's training and to achieve good cross-project prediction performance.

\subsubsection{Re-Balancing}

To build performing predictors, a quality training set must be fed to the machine learner. One important aspect is to have a balanced dataset, which means having as much as possible an equal number of positive and negative examples, in our context, as many \ac{SATD} tagged methods as clean methods. This is a serious problem in our dataset since the vast majority of methods are not technical debts (this will be discussed more in depth in the next section), only a minority contain \ac{SATD}. There are two ways to address this issue: under-sample the majority class (methods with no \ac{SATD}) or over-sample the minority class (\ac{SATD} tagged methods). Since the training set is so highly unbalanced, the number of instances of the minority class is excessively small, the second option was favored. In fact, under-sampling would result in a very small training set. To apply over-sampling, artificial instances of \ac{SATD} methods must be generated from the existing ones. To do so, Weka provides a tool to perform over-sampling, called\ac{SMOTE} \citep{chawla2002smote}, which combines under-sampling the majority class and over-sampling the minority class to achieve improved classifier performance.

\subsection{Building and Applying Machine Learning Models}

%0.5 page
 We extracted various metrics, we identified \ac{SATD} methods and we performed a preprocessing on features, the only remaining step is building and applying the machine learning models. Two sets are required: the training set and testing set. The training set contains the methods with their corresponding features and an additional variable to tag them as positive or negative (\ac{SATD} or not). It is used to build the models. The testing set contains the same methods with the corresponding features, but not the variable tagging the methods because we want to test the prediction performance from a blank dataset. We experimented with five types of machine learners in Weka \citep{hall2009weka}: Decision Trees (J48), Bayesian classifiers, Random Forests, Random Trees, and Bagging with Decision Trees. Only the default configurations were used, however, further work could be made by trying to optimize these configurations. Here is a little overview of each algorithms: \par 

\begin{itemize}
\item \textit{Decision trees}, namely J48, which implements the standard C4.5 algorithm using the concept of information entropy \citep{Quinlan1993}.
\item \textit{Bayesian classifiers}, which apply the Bayes' theorem to classify observations, assuming strong independence between features. More specifically, we use BayesNet in Weka, which is the base class for Bayes Network classifiers. It provides datastructures and facilities common to learning algorithms K2 and B.
\item \textit{Random forests}, which averages multiple decision trees trained on different parts of a same training set, with the aim to reduce the variance of the classifications and the risk of overfitting to the training set \citep{Breiman2001}.
\item \textit{Random trees}, which build decision trees considering a number of randomly chosen attributes at each node.
\item \textit{Bagging}, which combines multiple classifiers (decision trees in our case) built on random samples of the training set \citep{Breiman1996}. It was designed to improve the stability and accuracy of machine learning algorithms, to reduce the variance and to avoid overfitting.
\end{itemize}

 
\begin{landscape}
 \begin{figure}[t]
 	\centering
 	\includegraphics[width=\linewidth]{figs/MachineLearning.pdf}
 	\caption{Process for building and applying machine learners.}
 	\label{fig:MLProcess}
 	\vspace{-4mm}
 \end{figure}
\end{landscape}
 
 %\textbf{SHOW IMAGE OF PSEUDOCODE AND EXPLAIN IT. VOIR LE ECLIPSE PROJECT QUE FIORELLA A ENVOYE.} 
 
 Figure \ref{fig:MLProcess} provides an overview of the machine learning process. To increase its speed, several threads are created. For the 10 fold cross-validation inter-project analysis, one thread is started for each fold, which were generated beforehand for each software system. The following actions are performed simultaneously on each thread and the computation is finished once each of them are finished. All possible combinations of machine learners and configurations are trained and tested on each fold. We have 5 different types of machine learners, the possibility of balancing the training set or not, and the possibility of applying feature selection or not, for a total of 20 predictors. \par
 
 The evaluation step consists of a training and testing phase. In the training phase, the variables to predict are set, \emph{i.e} \ac{SATD} methods, and feature preprocessing is applied. The training set can be balanced or not, data is normalized, useless features are removed and feature selection is applied or not. The models can then be trained. In the testing phase, the variables to predict are set again, as for the feature preprocessing, only data normalization is performed. The classifiers are tested and then the confusion matrix and the AUC value computed. Results are finally printed to perform further analysis.
 
\section{Study Definition}

%0.5 page

The goal of this thesis is to assess the prediction performance of our machine learning based approach in recommending technical debts to self-admit. The focus is to enhance source code quality, more specifically its maintainability and understandability, by keeping track of technical debts which can be corrected in the future. The perspective of this thesis is to be able to suggest to developers when to admit technical debts. We aim at addressing three research questions:

\begin{itemize}
	\item \textbf{RQ1}: How does \ac{TEDIOUS} work for recommending \ac{SATD} within-project?
	\item \textbf{RQ2}: How does \ac{TEDIOUS} work for recommending \ac{SATD} across-project?
	\item \textbf{RQ3}: How would a method-level smell detector compare with \ac{TEDIOUS}?
\end{itemize}

\subsection{Dataset}

%2 pages

\begin{landscape}
\begin{table*}[t]
	\caption{Characteristics of the studied projects.}
	\label{tab:projects}
	\centering
		\begin{adjustbox}{center}
			\begin{tabular}{l r | r r r r | r | r r | r}
				\hline
				\multirow{2}{*}{Project} & \multirow{2}{*}{Release} &\multicolumn{4}{c|}{Number of} &\multicolumn{1}{c|}{Number of Comments}
				&\multicolumn{2}{c|}{Number of Design SATD} & \% of Methods\\
				&& Files& Classes& Methods& Comments                      & $\in$ Methods& $\notin$ Methods & $\in$ Methods & with design SATD\\
				\hline
				Ant&1.7.0 & 1,113 & 1,575 & 11,052 & 20,325               & 13,359        &  1 & 57 & 0.5\% \\
				ArgoUML&0.34& 1,922 & 2,579 & 14,346 & 64,393       &  17,722       & 203 & 425  & 2\%\\
				Columba&1.4& 1,549 & 1,884 & 7,035 & 33,415           & 10,305        & 8 & 418 & 5\%  \\
				Hibernate&3.3.2 GA & 2,129 & 2,529 & 17,405 & 15,901 & 9,073        & 21  &  377  &  2\%\\
				jEdit & 4.2 & 394 & 889 & 4,785 & 15,468                     &10,894         & 6  & 77  & 2\% \\
				jFreeChart&1.0.19 & 1,017 & 1,091 & 10,343 & 22,827  & 15,412       &  4  & 1,881  & 18\%\\
				jMeter&2.1& 1,048 & 1,328 & 8,680 & 19,721                &  12,672      & 95 &  424 & 5\%  \\
				jRuby&1.4.0 & 970 & 2,063 & 14,163 & 10,599               & 7,809        & 16   & 275  &  2\%\\
				Squirrel&3.0.3 & 2,325 & 4,123 & 16,648 & 25,216         & 15,574      &35  & 173  & 1\%\\
				\hline
			\end{tabular}
		\end{adjustbox}
		\vspace{-4mm}
\end{table*}
\end{landscape}

To evaluate our approach, we used a dataset that was already analyzed to find \ac{SATD} methods \citep{maldonado17}. It consists of ten open source projects, although we used only nine of them since we could not download the specific version of EMF, where \ac{SATD} were detected using a \ac{NLP} approach. The methods were then manually validated and classified. Table \ref{tab:projects} summarizes the characteristics of all studied projects. It provides information on project releases; number of files, classes, methods and comments in projects; number of comments in methods; number of design \ac{SATD} in methods and not in methods; and percentage of methods with design \ac{SATD}. \par

Some differences were observed with the characteristics extracted from the original paper \citep{maldonado17}, concerning the number of classes, methods and comments. Several reasons can explain these disparities: different extraction tooling, tools characteristics and processing. For example, we considered each comment as a single entity, whereas \citet{maldonado17} regrouped successive line comments. Additionally, we did not establish a separation between classes and their inner classes, and we considered interfaces as classes. Methods related to inner classes were associated to its container. However, these differences are not an issue for our research since they concern classes and our approach is method-level based. Some files from \citet{maldonado17} analysis could have been left aside because of their absence of comments. \par 

We can obtain some interesting information when looking at Table \ref{tab:projects}. As explained previously, we clearly see a prevalence of method-related \ac{SATD} compared to class-level \ac{SATD}. They are at least 2 times more common for \textsc{ArgoUML}, which contains about half of all the class-level \ac{TD} in the dataset, and can be up to 470 times more common for \textsc{jFreeChart}, only 4 out of the 1,885 design \ac{SATD} are at class-level. Globally, we are around 10 times more likely to encounter a method-level design technical debt in our dataset than class-level. We also observe that the dataset is highly unbalanced between \ac{SATD} prone and non \ac{SATD} prone methods. \textsc{jFreeChart} provides a decent ratio with 18\% of methods containing a design \ac{TD} but all other projects have 5\% or less of their methods containing design debts. To put this into perspective, out of the 11,052 methods in \textsc{Ant}, only 57 are \ac{SATD} prone. For \textsc{jEdit}, only 77 instances out of 4,785 are proned to contain a \ac{SATD}. Unsurprisingly, as we will discuss in the analysis of study results, these two projects achieved the lowest performance values. \par 

The replication package provided by \citet{maldonado17} contains information on \ac{SATD} at class-level. However, to build the oracle, we need to assign \ac{SATD} at method-level. To do so, we performed pattern matching between known \ac{SATD} comments and comments attached to methods. If a comment is matched inside a class but not a method, it is attached to the class, if it is matched outside of a class, it is attached to the file. These class-level and file-level technical debts are not considered in our research, which is not a big issue since they represent the minority.

\subsection{Analysis Method}

%3.5 pages

For \textbf{RQ1}, we want to know how \ac{TEDIOUS} works for recommending \ac{SATD} within-project. A 10-fold cross validation was performed on each project. In other terms, the dataset of a single project is divided into 10 folds, the machine learner is trained on 9 of them and tested on the remaining one, until all 10 configurations are processed to limit the effect of randomness. The performance values are averaged over the 10 iterations to obtain the most representative picture. For \textbf{RQ2}, we want to know how our approach works for recommending \ac{SATD} across-project. The process is similar to \textbf{RQ1}, but instead we train on 8 projects and test on the remaining one, until all the 9 possible combinations are executed. \par 

Standard performance metrics for evaluating automated classification were computed to analyze our approach: precision, recall and $F_{1}$ score. These metrics were computed for the \ac{SATD} category. \par 

\ac{Pr} is the percentage of relevant instances of methods predicted as \ac{SATD} among all retrieved instances. \ac{TP} and \ac{FP} are the number of true positives, correct methods predicted as \ac{SATD}, and false positives, incorrect methods predicted as \ac{SATD}.

\[
Pr=\frac{TP}{TP+FP}
\]

\ac{Rc} is the percentage of relevant instances of \ac{SATD} methods that have been retrieved over all relevant instances. \ac{FN} is the number of false negatives, incorrect methods predicted as non \ac{SATD}.

\[
Rc=\frac{TP}{TP+FN}
\]

The $F_{1}$ score is the harmonic mean between precision and recall, which provides a single measurement for evaluating a system.

\[
F_1=2 \cdot \frac{PR \cdot RC}{PR + RC}
\]

The previous metrics are specific to the \ac{SATD} class, which means true negatives \ac{TN}, correct methods predicted as non \ac{SATD}, are not considered. Consequently, other metrics are required to complement the analysis: accuracy, Matthews Correlation Coefficient (MCC) \citep{matthews1975comparison}, and the Area under the Receiving Operating Characteristics (ROC) Curve (AUC). \par 

\ac{Acc} is the total number of methods correctly predicted, whether it is containing a \ac{SATD} or not, among all the methods analyzed.

\[
Acc=\frac{TP+TN}{TP+TN+FP+FN}
\]

The \ac{MCC} is a metric used in machine learning to evaluate the quality of a two-class classifier. It is especially useful when the dataset is unbalanced \citep{matthews1975comparison}. Values vary between -1 for a completely wrong classifier and 1 for a completely correct classifier. 

\[
MCC=\frac{TP \cdot TN-FP \cdot FN}{\sqrt{(TP+FP)(FN+TN)(FP+TN)(TP+FN)}}
\]

The \ac{ROC} curve is created by plotting the true positive rate against the false positive rate at various classifier thresholds. The \ac{AUC} is the area under the ROC curve, it provides a value to evaluate the quality of the classifier. A value of AUC=0.5 refers to a random classifier and the higher the value, the better the classifier.

To have a good idea of the performance of each machine learner, each of the previous metrics have to be considered in the evaluation. We want a balance between precision and recall because we want as much as possible to detect real technical debts and all of them. We cannot only use $F_{1}$ score because we want to take into account the effect of chance on the predictions. Thats is why we also computed the MCC and AUC values.

In addition to these performance indicators, we also consider the importance of each features for the training of the predictors. We used a technique implemented in Weka for Random Forests named \ac{MDI} \citep{louppe2013understanding}, which measures the importance of variables on randomized decision trees.

For \textbf{RQ3}, we want to compare \ac{TEDIOUS} with a popular method-level smell detector (DECOR) \citep{moha2010decor} in classifying as technical debt methods labeled as \ac{SATD}. DECOR can detect a large amount of smells, but most of them are at class-level, which are not relevant with the level at which \ac{TEDIOUS} works. Instead of using all of them, we narrowed our analysis on two method-level smells, \textit{Long Method} and \textit{Long Parameter List}. To identify a Long Method smell, DECOR follows the rule {\em LOC}$>th_1$ where $>th_1$ is a threshold for the LOC. To identify a Long Parameter List smell, DECOR follows the rule {\em ParNbr}$>th_2$ where {\em ParNbr}$>th_2$ is a threshold for the number of parameters. Various thresholds for LOC and ParNbr were considered in our research, between percentile 0.5 and 0.95, as well as the default thresholds, namely percentile 0.75 for LOC and outlier (third quartile $+ 1.5 \cdot IQR$ (interquartile range)) for ParNbr.

To finish, we performed a qualitative analysis on false positives and false negatives examples we obtained when evaluating our predictors. Its purpose is to complement our quantitative analysis and discuss the limitations of our approach in recommending \ac{TD} with real examples.
























