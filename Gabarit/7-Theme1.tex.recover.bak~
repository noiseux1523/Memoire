\Chapter{THE APPROACH AND STUDY DEFINITION}\label{sec:Theme1}

%TOTAL = 15 pages

\section{The Approach}

%2 pages

This section will describe the steps followed to design \ac{TEDIOUS}, our proposed machine learning detector to identify design technical debts to self-admit. It will also define its characteristics, how it works and how to use it. \ac{TEDIOUS} works at method-level since it is typically the granularity at which developers introduce \ac{SATD}, as proven by \citet{MaldonadoS15} and \citet{PotdarS14}. In other terms, it is able to define whether a method contains a design technical debt or not. Class-level granularity would be to coarse because technical debts normally admitted by developers are related to specific and punctual issues in the source code. Additionally, a class could contain a \ac{TD} but it would be impossible to precisely identify the source of the problem since a class contains several methods and \ac{LOC}. \par 

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/approach.pdf}
	\caption{Proposed approach for recommending SATD.}
	\label{fig:approach}
	\vspace{-4mm}
\end{figure}

\ac{TEDIOUS} works as shown in Figure \ref{fig:approach}. Two datasets are required as inputs: the training set and the test set. The training set contains labeled data, which his source code from a project where technical debts are known and have been self-admitted through comments. The test set contains unlabeled data, which can be any source code under development or already released where \ac{TEDIOUS} can attempt to recommend where \ac{TD} should be self-admitted or where source code should be improved. \par 
	
For the training set, various kinds of metrics and static analysis warnings are extracted from the source code as well as \ac{SATD} methods in order to build an oracle to train the model. These labeled \ac{SATD} methods are essential for the machine learner since supervised learning is performed, meaning each method is labeled as true or false. It is true when the method is a \ac{TD} and false if it is not. \par 

Once all the information is extracted, feature preprocessing and selection is applied. Multi-collinearity, a phenomenon occurring when two predictor variables are highly correlated, meaning that one can be linearly predicted by the other, is dealt with. Feature selection is applied to retain only the most relevant variables to train the predictor. Finally, re-balancing is performed to address the issue of the low amount of positive examples, \textit{i.e.} \ac{SATD} methods. With the preprocessed features and the oracle now defined (each method is labeled as \ac{SATD} or not), the machine learners can be trained. \par 

In parallel, the test set is also being prepared. The same features are extracted from the source code but no \ac{SATD} matching is required since the data is unlabeled. \ac{SATD} are only required for the oracle, which is used for training the models. A similar feature filtering is applied, except for the re-balancing since it is only required on the labeled data. With both the test set and the previously trained classifier, predictions can be made on the test set in order to recommend when to self-admit technical debts. \par

Each step of the process will be described in the following sections. Firstly, the features used in more details: source code metrics and warnings raised by static analysis tools. Secondly, the method employed to identify \ac{SATD}. Thirdly, the feature preprocessing: multi-collinearity, feature selection and re-balancing. Finally, the training and application of the machine learning models.

\subsection{Features}

%3 pages

Three pieces of information extracted from the source code are necessary to accurately describe the source code: structural metrics, readability metrics and warnings raised by static analysis tools. There are reasons these specific characteristics were retained to describe the source code. The structural metrics are essential to capture symptoms of complex, heavily couples and poorly designed code. These metrics explain the quality of the implementation and the software's design. The readability metrics quantify symptoms of poorly documented code or hard to read and understand. Warnings from static analysis tools are related to more specific bad choices rather than globally bad code. They are issues which could lead to low maintainability and understandability of the code or which could potentially introduce defects in the future. In the following sections, these metrics and warnings are described more in depth.

\subsubsection{Source Code Metrics}

Source code metrics are extracted to characterize its size, coupling, complexity and number of comments for readability. Nine source code metrics are used. To define the size, metrics like \ac{LOC} or number of statements are calculated. For coupling, a metric such as number of call sites is computed. For complexity, McCabe cyclomatic complexity \citep{mccabe1990reverse}, number of defined variables, number of expressions and number of identifiers are calculated. It is important to know that not all comments are considered. \ac{SATD} related comments are ignored in the empirical evaluation to avoid \ac{TEDIOUS} becoming a self-prophecy. This issue will be covered later in the study design. \par

Firstly, to extract those source code metrics, an XML representation of the Java source code was generated using the srcML tool \citep{CollardKM03} and metrics' computation was applied to it. \textbf{SHOW IMAGE OF XML REPRESENTATION. SHOW IMAGE OF PSEUDOCODE AND EXPLAIN IT. VOIR PDF DU COURRIEL DANS DROPBOX. CODE DE GIULIO.} It was also required to link comments to their related methods. The rule used was that any comments directly preceding or inside a method was assigned to it, rather it be a block of comments or just a single line. This step is essential to be able to classify which method is a \ac{SATD} and which one is not. Some methods were excluded from our analysis because they did not fit the context of our study, namely getter and setter methods since they are irrelevant with design debts. To do so, we looked for method prefixes matching \emph{get} or \emph{set} and made up from at least a single line of code. When these two criteria were covered, the method was removed from the dataset. \par 

%Using  the {\em  srcML} tool, we processed each Java file, and stored the XML Java representation. We then used the Python ElementTree XML package to build the file level document object model; visiting the DOM and extracting methods along with  the attached comments and computing a set of structural metrics \Foutse{are these metrics different from those described in the approach section?}. We assigned to a method the method proceeding comments not separated from the method definition by any other statement. Then, as also described in Section \ref{sec:approach}, for each method we extracted source code structural metrics, the Buse and Weimer readability metric, and then we executed CheckStyle and PMD on each source code file. Finally, we mapped CheckStyle and PMD warnings onto methods, by knowing the line(s) in which a warning occurred and the line span of each method.

Secondly, to extract the code readability metric, we use the one proposed by \citet{Buse:tse2010} and its related tool, based on a machine learning approach. This metric is based on specific characteristics of the source code: indentation, line length, identifier length, comment density, and the use of keywords and operators. The tool was also designed using feedback from human annotators, they were asked to rate the readability of code snippets, that was then used to classify snippets as "less" or "more" readable. A value between 0 and 1 is then computed based on these characteristics, 1 being the highest readability. The readability metric was considered relevant because, as mentioned by \citet{BavotaR16}, code readability is strongly correlated with the introduction of technical debts. Code that is difficult to read is consequently difficult to maintain and understand \citep{Buse:tse2010} thus more likely to contain \ac{TD}. Finally, here are the nine source-code metrics we extracted:

\begin{itemize}
\item \textit{LOC:} Number of lines of code in the body of a method. % (excluding comments).
\item \textit{Number of statements: } Number of occurrences of expression statements in a method. In case of local class definitions, the number of statements in the enclosing method is increased by the total number of statements of the local class.
\item \textit{Number of comments:} Number of single-line and multi-line comments in a method.
\item \textit{McCabe cyclomatic complexity:} Number of linearly independent paths of a method \citep{mccabe90}.
\item \textit{Number of passed parameters:} Number of parameters of a method.
\item \textit{Number of identifiers:} Number of unique identifiers in a method.
\item \textit{Number of call sites:} Number of locations in the code where zero or more arguments are passed to a method, or zero or more return values are received from the method.
\item \textit{Number of declarations:} Number of variable and class declaration in a method.
\item \textit{Number of expressions:} Number of expressions contained in a method.
\end{itemize}

\subsubsection{Warnings Raised by Static Analysis Tools}

Warnings raised by static analysis tools are essential to detect poor coding practices, which are also related to the introduction of technical debts. We cannot directly relate a single flagged practice to a \ac{TD}. However, we can wonder if several flags are justifiable or not, and if they can be caused by the presence of a \ac{TD} in the source code. Having this hypothesis in mind, we used two popular static analysis tools, namely CheckStyle and PMD. Firstly, CheckStyle \citep{checkstyle} is widely used to check the adherence to coding standards and pieces of code that are potentially smells. It performs its analysis based on a configuration file, which can be modified at the discretion of the users. For our study, the default configuration containing code styles defined by Oracle and 43 checks was used. PMD \citep{pmd} main goal is to find common programming flaws such as unused objects, unnecessary catch blocks or incomprehensible naming. Similarly to CheckStyle, the default configuration was used, which contains 168 checks. Several reasons explains the choice of these two static analysis tools: they are adopted by \ac{OSS} \citep{BellerBMZ16}, they provide a wide range of warnings related to code styles and programming practices and they can be executed on source code statically. It is important to know that \ac{SATD} comments were removed when performing the analysis.

\subsection{Identification of Self-Admitted Technical Debt}

%0.5 page

As mentioned previously, the purpose of this study is not to propose a novel approach to detect \ac{SATD} using information from comments. Previous work has been completed aiming to address this issue by using pattern matching \citep{MaldonadoS15}, \citep{PotdarS14} or \ac{NLP} combined with machine learners \citep{maldonado17}. However, we still needed a dataset with methods tagged as design debt to train our machine learning models. \citet{maldonado17}, which worked on the \ac{NLP} approach, published a dataset of 10 open source projects annotated with methods tagged as technical debt or not. We used this dataset for our machine learning models and only the design debts were retained. \par

Some preprocessing had to be done on the dataset since it reported \ac{SATD} at file-level and not method-level. To link \ac{SATD} to methods, we matched the \ac{SATD} comment string to the comments attached to methods. We used pattern matching in order to achieve this result, making sure that method comment strings completely matched \ac{SATD} comments before tagging it as a technical debt.

%Since the dataset reports SATD at file level, we attributed SATDs to methods by matching the SATD comment strings  onto comments attached to methods. In doing so we relied on pattern patching making sure that the SATD prone comment was exactly matched including the line order.

\subsection{Feature Preprocessing}

%3 pages

Several characteristics have been extracted from the source code, however, not all of them are relevant or necessary to train our models. Some clean up have to be done to reduce the size of the data input and improve the training phase. Multi-collinearity have to be dealt with, feature selection as to be applied as well as training set re-balancing. \textbf{SHOW IMAGE OF PSEUDOCODE AND EXPLAIN IT. FAIRE UN GRAPH STYLE LE PROCESS AU COMPLET. UN CARRE = UNE SOUS SECTION.}

\subsubsection{Multi-Collinearity}

We computed a lot features to characterize the source code of the projects, however, some of them can be strongly correlated and vary in the same way. Keeping these pairs of features would cause redundancy since they can be mutually linearly predicted. That is why, when facing a duo of such features, we only keep the one that better correlates with the dependent variable, the \ac{SATD} methods. The \emph{varclus} function in R, from the \emph{Hmisc} package, was used to help us achieve this preprocessing. This function performs a hierarchical cluster analysis of features to detect when two variable are positive based on similarity measures. It is mainly used to assess collinearity and redundancy, consequently resulting in data reduction. Hoeffding D statistic, squared Pearson or Spearman correlations measures can be applied. In this study, the Spearman's $\rho$ rank correlation measure was retained. To identify the problematic duos, the cluster tree has to be cut at a particular level of $\rho^2$, which represents the correlation value. In our case, a value of $\rho^2=0.64$ was used since it corresponds to a strong correlation \citep{Cohen-1988}.

\subsubsection{Feature Selection and Normalization}

Some features will vary a lot or not at all between methods. These features are not useful to build a predictor because they can't be used as proper learning variables, they won't be determinant compared to all the other features. The process of selecting only a relevant subset of all possible features is called feature selection. Several reasons can justify going through this selection: improving the prediction performance of machine learning models, improve the speed and cost-effectiveness of predictors, and simplify the models to better understand and interpret the underlying process behind the dataset generation \citep{guyon2003introduction}. \par 

The \emph{RemoveUseless} filter implemented in Weka \citep{hall2009weka} was used to perform feature selection. It looks for features that never vary or features that vary too much. For the latter, it looked for features that had a percentage of variance above a specific treshold, which was set to 99\%. \par 

In addition to feature selection, the metrics were also normalized. Several projects were analyzed, all having significant differences in complexity and size characteristics. Those aspects of the dataset will be covered in the study definition section. This normalization is necessary to reduce the effect of those differences on the predictor's training and to achieve good cross-project prediction performance.

\subsubsection{Re-Balancing}

To build performing predictors, a quality training set must be fed to the machine learner. One important aspect is to have a balanced dataset, which means having as much as possible an equal number of positive and negative examples, in our context, as many \ac{SATD} tagged methods as clean methods. This is a serious problem in our dataset since the vast majority of methods are not technical debts (this will be discussed more in depth in the next section), only a minority contain \ac{SATD}. There are two ways to address this issue: under-sample the majority class (methods with no \ac{SATD}) or over-sample the minority class (\ac{SATD} tagged methods). Since the training set is so highly unbalanced, the number of instances of the minority class is excessively small, the second option was favored. In fact, under-sampling would result in a very small training set. To apply over-sampling, artificial instances of \ac{SATD} methods must be generated from the existing ones. To do so, Weka provides a tool to perform over-sampling, called\ac{SMOTE} \citep{chawla2002smote}, which combines under-sampling the majority class and over-sampling the minority class to achieve improved classifier performance.

\subsection{Building and Applying Machine Learning Models}

%0.5 page
 We extracted various metrics, we identified \ac{SATD} methods and we performed a preprocessing on features, the only remaining step is building and applying the machine learning models. Two sets are required: the training set and testing set. The training set contains the methods with their corresponding features and an additional variable to tag them as positive or negative (\ac{SATD} or not). It is used to build the models. The testing set contains the same methods with the corresponding features, but not the variable tagging the methods because we want to test the prediction performance from a blank dataset. We experimented with five types of machine learners in Weka \citep{hall2009weka}: Decision Trees (J48), Bayesian classifiers, Random Forests, Random Trees, and Bagging with Decision Trees. Only the default configurations were used, however, further work could be made by trying to optimize these configurations. \par 

\begin{itemize}
\item \textit{Decision trees}, namely J48, which implements the standard C4.5 algorithm using the concept of information entropy \citep{Quinlan1993};
\item \textit{Bayesian classifiers}, which apply the Bayes' theorem to classify observations, assuming strong independence between features. More specifically, we use BayesNet in Weka, which is the base class for Bayes Network classifiers. It provides datastructures and facilities common to learning algorithms K2 and B;
\item \textit{Random forests}, which averages multiple decision trees trained on different parts of a same training set, with the aim to reduce the variance of the classifications and the risk of overfitting to the training set \citep{;
\item \textit{Random trees}, which build decision trees considering a number of randomly chosen attributes at each node.
\item \textit{Bagging} combines multiple classifiers (decision trees in our case) built on random samples of the training set.
\end{itemize}

 
\begin{landscape}
 \begin{figure}[t]
 	\centering
 	\includegraphics[width=\linewidth]{figs/MachineLearning.pdf}
 	\caption{Process for building and applying machine learners.}
 	\label{fig:MLProcess}
 	\vspace{-4mm}
 \end{figure}
\end{landscape}
 
 %\textbf{SHOW IMAGE OF PSEUDOCODE AND EXPLAIN IT. VOIR LE ECLIPSE PROJECT QUE FIORELLA A ENVOYE.} 
 
 Figure \ref{fig:MLProcess} provides an overview of the machine learning process. To increase its speed, several threads are created. For the 10 fold cross-validation inter-project analysis, one thread is started for each fold, which were generated beforehand for each software system. The following actions are performed simultaneously on each thread and the computation is finished once each of them are finished. All possible combinations of machine learners and configurations are trained and tested on each fold. We have 5 different types of machine learners, the possibility of balancing the training set or not, and the possibility of applying feature selection or not, for a total of 20 predictors. \par
 
 The evaluation step consists of a training and testing phase. In the training phase, the variables to predict are set, \emph{i.e} \ac{SATD} methods, and feature preprocessing is applied. The training set can be balanced or not, data is normalized, useless features are removed and feature selection is applied or not. The models can then be trained. In the testing phase, the variables to predict are set again, as for the feature preprocessing, only data normalization is performed. The classifiers are tested and then the confusion matrix and the AUC value computed. Results are finally printed to perform further analysis.
 
\section{Study Definition}

%0.5 page

The goal of this study is to assess the prediction performance of our machine learning based approach in recommending technical debts to self-admit. The focus of this study is to enhance source code quality, more specifically its maintainability and understandability, by keeping track of technical debts which can be corrected in the future. The perspective of this study is to be able to suggest to developers when to admit technical debts. We aim at addressing three research questions:

\begin{itemize}
	\item \textbf{RQ1}: How does \ac{TEDIOUS} work for recommending \ac{SATD} within-project?
	\item \textbf{RQ2}: How does \ac{TEDIOUS} work for recommending \ac{SATD} across-project?
	\item \textbf{RQ3}: How would a method-level smell detector compare with \ac{TEDIOUS}?
\end{itemize}

\subsection{Dataset}

%2 pages

To evaluate our approach, we used a dataset that was already analyzed to find \ac{SATD} methods \citep{maldonado17}. It consists of ten open source projects, although we used only nine of them since we could not download the specific version of EMF, where \ac{SATD} were detected using a \ac{NLP} approach. The methods were then manually validated and classified. 

\subsection{Analysis Method}

3.5 pages














