\Chapter{ANALYSIS OF STUDY RESULTS AND THREATS TO VALIDITY}\label{sec:Theme2}

%TOTAL = 18 pages

\section{Study Results}

This section reports the study results in the context of each research question. Tables provide visual representation and summarize the main results. More in depth analysis is discussed for the three research questions and the qualitative analysis.

\subsection{How does TEDIOUS work for recommending SATD within-project?}

%4.5 pages

\begin{table}[t]
	\caption{Average performance of different machine learners for within-project prediction.}
	\label{tab:avgWithin}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lrrrrrr}
			\multicolumn{7}{c}{ \textbf{Without Balancing}}\\
			\hline
			\textbf{ML} & \textbf{Pr} & \textbf{Rc} & \textbf{F$_{1}$} &\textbf{Acc} &\textbf{MCC} &\textbf{AUC}\\
			\hline
			\rowcolor{grey}
			\textbf{Random Forests} &49.97 &52.19&47.15&93.32&0.47&0.92\\
			\textbf{Bagging} &51.91 &48.45	&45.97&93.35&0.45&0.92\\
			\textbf{Bayesian} & 24.29&78.77&34.18&89.01&0.38&0.93\\
			\textbf{j48} & 34.86&54.42&39.54&94.18&0.39&0.82\\
			\textbf{Random Trees} &23.09&52.49&29.96&90.35&0.30&0.73\\
			\multicolumn{7}{c}{\textbf{With Balancing}}\\
			\hline
			\textbf{ML} & \textbf{Pr} & \textbf{Rc} & \textbf{F$_{1}$} &\textbf{Acc} &\textbf{MCC} &\textbf{AUC}\\
			\hline
			\rowcolor{grey}
			\textbf{Random Forests} & 26.56&68.26&36.04&90.45&0.37&0.92\\
			\textbf{Bagging} & 18.4&75.12&28.24&85.58&0.31&0.90\\
			\textbf{Bayesian} & 4.00&94.07&7.55&15.66&0.04&0.72\\
			\textbf{j48} & 16.95&77.76&26.45&84.04&0.30&0.85\\
			\textbf{Random Trees} &16.03&63.22&24.49&85.34&0.26&0.75\\
			\hline
	\end{tabular}}
	\vspace{-3mm}
\end{table}

Table \ref{tab:avgWithin} presents the average performance results of a 10-fold cross validation within-project executed 10 times and on different machine learners. The average was computed for the 9 studied projects. The 10-fold cross validation was performed with balancing using SMOTE and without balancing.

On the unbalanced dataset, the best classifier is the one using the Random Forests algorithm. It achieves the best balance between precision ($49.97\%$) and recall ($52.19\%$), obtaining a $F_{1}$ score of $47.15\%$, the highest of all machine learners. We also notice that the Bagging algorithm is performing almost as well as Random Forests, even obtaining a slightly better precision but a weaker $F_{1}$ score. The accuracy of Random Forests, which includes the correct classification of negatives (the vast majority of the data), is $93.32\%$ and almost all the other machine learners obtain an accuracy higher than $90\%$, between $[89.01\%-93.35\%]$. MCC is on average $>0.4$, which is translated into a moderate correlation, and AUC is $>0.9$ (close to a perfect classifier) for Random Forests, Bagging and Bayesian and $>0.7$ for j48 and Random Trees.

On the balanced dataset, the best classifier is still Random Forests, with a precision of $26.56\%$, recall of $68.26\%$ and $F_{1}$ score of $36.04\%$. Its MCC value is the highest at $0.37$, moderate correlation, and the same goes for its AUC value which is the same as previously, $0.92$. The purpose of balancing is achieved since the recall of each machine learners is higher than previously, at the expense of precision. There is a clear gap between the Bayesian classifier and the others, it is definitely performing more poorly. All the performance values are the worst, except for the recall which is really good but not enough to compensate. In fact, it performs like a random classifier if we look at the MCC value which is almost $0$. The other classifiers all performed similarly, having a precision between $[16.03\%-18.40\%]$, a recall between $[63.22\%-75.12\%]$ and a $F_{1}$ score around $25\%$. Their MCC value is around $0.3$, which translates to a fair correlation and the AUC value is decent at $0.7$ or more. However, the results are globally weaker with balancing than without it.

\begin{table}[t]
	\caption{Within-project prediction: results of Random Forests for each system, without and with SMOTE balancing.}
	\label{tab:bysyswithin}
	\centering\scriptsize
	\resizebox{\linewidth}{!}{
		
		\begin{tabular}{lrrrrrr}
			%\hline
			\multicolumn{7}{c}{ \textbf{Without Balancing}}\\
			\hline
			\textbf{System} & \textbf{Pr} & \textbf{Rc }&\textbf{F$_{1}$} &\textbf{Acc} &\textbf{MCC} &\textbf{AUC}\\
			\hline
			Ant &0.91& 16.39 & 1.73 & 84.59 & 0.00 & 0.77\\
			ArgoUML &85.19& 38.10 & 52.65 & 93.25 & 0.54&0.91\\
			Columba & 36.40 & 65.94 & 46.91 & 96.02 & 0.47 & 0.94\\
			Hibernate & 53.44 & 65.22 & 58.74 & 96.80 & 0.57 & 0.97\\
			jEdit & 5.24 & 25.71 & 8.71 & 85.51 & 0.06 & 0.81\\
			jFreeChart & 84.58 & 82.52 & 83.54 & 98.91 & 0.83 & 0.99\\
			jMeter & 53.38 & 47.37 & 52.30 & 96.69 & 0.51 & 0.94\\
			jRuby & 52.27 & 84.02 & 64.45 & 94.21 & 0.64 & 0.97\\
			Squirrel & 73.33 & 44.44 & 55.35 & 99.51 & 0.57 & 0.97\\
			\multicolumn{7}{c}{ \textbf{With Balancing}}\\
			\hline
			\textbf{System} & \textbf{Pr} & \textbf{Rc }&\textbf{F$_{1}$} &\textbf{Acc} &\textbf{MCC} &\textbf{AUC}\\
			\hline
			Ant & 2.46& 44.26 & 4.67 & 85.02 & 0.08 & 0.83\\
			ArgoUML &47.03 & 65.39 & 54.71 & 89.34 & 0.50&0.90\\
			Columba & 15.35 & 74.64 & 25.46 & 88.35 & 0.30 & 0.94\\
			Hibernate & 19.85 & 89.13 & 32.47 & 87.04 & 0.38 & 0.95\\
			jEdit & 7.74 & 34.29 & 12.63 & 87.25 & 0.11 & 0.86\\
			jFreeChart & 62.98 & 92.68 & 75.00 & 97.94 & 0.75 & 0.99\\
			jMeter & 32.03 & 64.47 & 42.79 & 93.40 & 0.42 & 0.92\\
			jRuby & 32.75 & 91.91 & 48.29 & 87.72 & 0.50 & 0.92\\
			Squirrel & 18.81 & 57.58 & 28.36 & 98.02 & 0.32 & 0.96\\
			\hline
		\end{tabular}
	}
	\vspace{-3mm}
\end{table}

Table \ref{tab:bysyswithin} highlights the within-project prediction results, for each system, using Random Forests, and using balancing or not. Random Forests only was used since it is best classifier based on Table \ref{tab:avgWithin}. If we look at the unbalanced dataset, two systems are performing way worse than the others, namely \textsc{Ant} and \textsc{jEdit}. There is a reason behind this if we look back at Table 3.1. The projects, other than \textsc{jFreeChart} with 18\%, all have a percentage of their methods containing SATD below or equal to 5\%. \textsc{Ant} only has 0.5\% of its methods containing SATD and \textsc{jEdit} 2\%. This explains the low performance values of \textsc{Ant} (precision $0.91\%$, recall $16.39\%$ and $F_1$ score $1.73\%$) and \textsc{jEdit} (precision $5.24\%$, recall $25.71\%$ and $F_1$ score $8.71\%$). The AUC values are still decent, respectively with $0.77$ and $0.81$, but the MCC values, respectively with $0$ and $0.06$, clearly prove us that the classifier is as good as a random one.

\textsc{jFreeChart} is the project containing the most number of SATD and is consequently the project where the classifier performs the best. It obtains high precision and recall ($84.58\%$ and  $82.52\%$) as well as high MCC and AUC ($0.83$ and $0.99$). Performance values on the other 6 projects are also promising, the $F_1$ score is almost always $>50\%$, the MCC is between $[0.47-0.64]$ which is moderate to strong correlation, and the AUC is in the interval $[0.91-0.97]$. We notice that the prediction performance of TEDIOUS is dependent on the system and not only the number of SATD it is trained on. \textsc{Squirrel} has a slightly higher percentage of SATD methods than \textsc{Ant} and slightly lower than \textsc{jEdit}, but it still performs significantly better than these two projects ($73.33\%$ precision and $44.44\%$ recall). 

If we look at the balanced dataset, the same trend is observed as in the balanced dataset but with lower performance results. Precision is generally lower except for \textsc{Ant} and \textsc{jEdit} which obtain a small improvement. These two systems should normally benefit from balancing but the data from the few SATDs is not even enough to build a decent artificial training set, leading to a negligible gain in precision. However, as expected with balancing, we see a decent increase in their recall and the same goes for the other systems. Generally speaking, the accuracy, $F_1$ score, MCC and AUC is better for \textsc{Ant} and \textsc{jEdit} only, the other systems did not benefit from the balancing.

\begin{table*}[t]
	\caption{Top 10 discriminant features (within-project prediction). (M): source code metrics,  (CS): CheckStyle checks, (P): PMD checks.}
	\label{tab:top10features}
	\centering\scriptsize
	\resizebox{\linewidth}{!}{
		
			\begin{tabular}{lcccccccccc}
				Metric Name &Ant&ArgoUML&Columba&Hibernate&jEdit&jFreeChart&jMeter&jRuby&Squirrel\\
				\hline
				\rowcolor{grey}
				Readability (R)&5&1&2&1&1&1&1&1&1\\
				LOC (M)&2&2&5&2&2&3&2&3&4\\
				%\hline
				\rowcolor{grey}
				DeclNbr (M)&4&3&7&4&3&4&3&4&3\\
				ParNbr (M) &8&5&9&7&7&7&7&7&7\\
				%\hline
				%\hline
				%\hline
				\rowcolor{grey}
				ExprStmtNbr (M)&6&4&---&5&4&5&5&5&5\\
				%\hline
				McCabe (M)&10&7&---&6&6&6&6&6&6\\
				\rowcolor{grey}
				%\hline
				CommentNbr (M)&---&6&---&3&5&2&4&2&2\\
				%\hline
				LineLength (CS) &---&---&---&9&---&---&9&8&9\\
				%\hline
				\rowcolor{grey}
				LocalVariableCouldBeFinal (P) &---&---&---&10&9&---&---&9&10\\
				%\hline
				DataflowAnomalyAnalysis (P) &---&10&---&---&10&---&---&---&---\\
				%\hline
				\rowcolor{grey}
				FinalParameters (CS) &---&---&---&---&---&8&8&---&---\\
				%\hline
				MissingSwitchDefault (CS) &---&8&4&---&---&---&---&---&---\\
				\rowcolor{grey}
				%\hline
				AvoidReassigningParameters (P) &7&---&---&---&---&---&---&---&---\\
				%\hline
				CollapsibleIfStatements (P) &9&---&---&---&---&---&---&---&---\\
				%\hline
				\rowcolor{grey}
				EmptyIfStmt (P) &---&---&8&---&---&---&---&---&---\\
				%\hline
				IfStmtsMustUseBraces (P) &---&---&---&---&8&---&---&---&---\\
				%\hline
				\rowcolor{grey}
				LeftCurly (CS) &---&---&---&---&---&---&---&---&8\\
				%\hline
				LocalVariableName (CS) &---&---&1&---&---&---&---&---&---\\
				%\hline
				\rowcolor{grey}
				MethodArgumentCouldBeFinal (P) &---&---&---&---&---&---&---&10&---\\
				%\hline
				MethodLength (CS) &---&---&---&---&---&---&10&---&---\\
				%\hline
				\rowcolor{grey}
				OptimizableToArrayCall (P) &---&---&10&---&---&---&---&---&---\\
				%\hline
				ParameterNumber (CS) &---&---&---&---&---&10&---&---&---\\
				%\hline
				\rowcolor{grey}
				ParenPad (CS) &---&---&---&8&---&---&---&---&---\\
				%\hline
				ShortVariable (P) &---&---&---&---&---&9&---&---&---\\
				%\hline
				\rowcolor{grey}
				SimplifyBooleanReturns (CS) &---&9&---&---&---&---&---&---&---\\
				%\hline
				SwitchStmtsShouldHaveDefault (P) &---&---&6&---&---&---&---&---&---\\
				%\hline
				\rowcolor{grey}
				UselessParentheses (P) &3&---&---&---&---&---&---&---&---\\
				%\hline
				UseLocaleWithCaseConversions (P) &---&---&3&---&---&---&---&---&---\\
				%\hline
				\rowcolor{grey}
				UseStringBufferForStringAppends (P) &1&---&---&---&---&---&---&---&---\\
				%\hline
				%UseConcurrentHashMap&---&1&---&---&---&---&---&---&---\\
				%\hline
				%\rowcolor{grey}
				%UseObjectForClearerAPI&---&---&---&---&---&---&---&---&2\\
				%\hline
				%UseStringBufferForStringAppends&---&---&---&---&---&---&---&1&---\\
				%\hline
				%\rowcolor{grey}
				%WhitespaceAfter&---&---&10&---&---&---&---&---&---\\
				\hline
			\end{tabular}

	}
	\vspace{-2mm}
\end{table*}

Table \ref{tab:top10features} reports the top 10 features in within-project prediction according to the MDI technique.

\subsection{How does TEDIOUS work for recommending SATD across-project?}

4 pages

\subsection{How would a method-level smell detector compare with TEDIOUS?}

1.5 pages

\subsection{Qualitative discussion of false positive and false negatives}

4 pages

\section{Threats to Validity}

\subsection{Construct validity}

1 page

\subsection{Internal validity}

1 page

\subsection{Conclusion validity}

1 page

\subsection{External validity}

1 page

