\Chapter{CONCLUSION}\label{sec:Conclusion}

%TOTAL = 3 pages

% The conclusion helps draw attention to the thesis or dissertationâ€™s contribution to the advancement of knowledge and the development of technologies, while articulating its limits and constraints. It also serves as a space to identify areas for future research. Recommendations may also be set out here.


% Rechercher pour les mots "future/recommendation" pour les projets futurs
% Voir dans threats to validity pour changements a faire

%%
%%  SYNTHESE DES TRAVAUX
%%
\section{Summary of Work}

%1 page

This thesis describes two approaches to recommend to developers when they should self-admit design technical debts. The main approach is TEDIOUS, a machine learner that uses features extracted from source code as independent variables and knowledge of SATD as dependent variables to recommend TD to be self-admitted. It is based on size, complexity, readability and checks from static analysis tools. The second approach, less developed than TEDIOUS, is a convolutional neural network using source code directly to recommend TD to be self-admitted. It is based on source code and comments.

Both approaches were evaluated on 9 projects manually analyzed and made publicly available by \citet{maldonado17}. For TEDIOUS, the within-project prediction gave on average a precision of 50\%, recall of 52\% and accuracy of 93\% when recommending TDs. Oversampling was performed to compensate the highly unbalanced dataset (very few positive compared to negative instances) but without much benefits; it increased recall at the expense of precision. Cross-project prediction was also performed, with improved results compared to within-project. It gave on average a precision of 67\%, recall of 55\% and accuracy of 92\% when recommending TDs. The best systems achieved a precision and recall rate $> 88\%$, namely \textsc{jFreeChart} and \textsc{ArgoUML}. Code readability, size and complexity played a major role in recommending design SATD, with static analysis tools being less important.

For the CNN approach, four experiments were conducted with no balancing performed. For \textit{source code comments only} within-project prediction, an average precision of 93.63\%, recall of 76.17\% and accuracy 97.99\% were obtained. For \textit{source code with comments} within-project prediction, an average precision of 96.38\%, recall of 82.40\% and accuracy of 99.44\% were obtained. Adding the implemented code to the dataset improved the results. For \textit{source code without comments} within-project prediction, an average precision of 88.18\%, recall of 33.68\% and accuracy of 98.12\% were obtained. Performance metrics were negatively affected by the removal of all comments. For \textit{source code partially with comments} within-project prediction, an average precision of 94.84\%, recall of 58.83\% and accuracy of 98.82\% were obtained, which is a better than no comments but worst than all comments included. \textbf{ADD CROSS PROJECT RESULTS.} Overall, each CNN experiment performed better than TEDIOUS.

%%
%%  LIMITATIONS
%%
\section{Limitations of the Proposed Solution}\label{sec:Limitations}

% 1 page

TEDIOUS and our CNN approach have their share of limitations and constraints. The two approaches can't act as perfect classifiers because of the intrinsic nature of technical debts. You can try to describe methods with metrics and static analysis warnings, or use them as a whole, but there will always be a deeper level to their existence and a larger context that can be difficult for a machine learner to understand. We observed this aspect when qualitatively analyzing false negative examples of TEDIOUS. In other terms, the prediction performance of our proposed solution have a ceiling which is difficult to surpass.

For TEDIOUS, we are limited with the number of metrics and warnings we can use to train the machine learners. For the CNN, the number of LOC from the source code used for the dataset is also subject to a limit. The main reason behind this limitation is the processing time and computation power required (i) to extract these metrics and source code, and (ii) to train TEDIOUS and the CNN. We can't use as many training features as we want and have models trained in a decent amount of time with reasonable processing power.

In addition, there may be differences in feature values depending on the method to obtain them; the heavy preprocessing required to build datasets of TEDIOUS and the CNN approach can induce errors, and the manual classification of SATD comments is subject to human error. Also, default configurations only were used to collect warnings and train machine learners. Performance may be limited and could be increased by applying optimization on configuration parameters. Consequently, the results we obtained could be completely different, even though the same features are extracted, only because the process to extract them is different. In other terms, the replicability of our research is limited.

We can't assure that our results can be generalized to all Java projects even though we used systems previously studied \citep{maldonado17} which cover a wide variety of domains. Our dataset is limited, it only consists of 9 systems, and we cannot claim that TEDIOUS and the CNN can be accurate on every possible Java system. We already observe this fact in the various experiments we pursued. In addition, the applicability of our approaches is limited to Java programs only. Finally, the conclusion deduced from the analysis of the CNN results has to be taken with skepticism since it is still in its preliminary steps and the impact of randomness was not taken into account in its evaluation.

%%
%%  AMELIORATIONS FUTURES
%%
\section{Future Work}

%1 page

Future work can be divided in two themes: applicability scenarios and improvement paths. Applicability scenarios are ideas on how TEDIOUS and the CNN approach could be used concretely by developers in the industry. Improvement paths are tasks that have to be accomplished to improve and extend our approach.

For future applications, our approaches could act as recommendation systems for developers, suggesting when to self-admit technical debts with comments, which was the main goal of our research. Also, they could complement static analysis tools by helping to customize warning checks raised by them. Our machine learners could learn rules from previously detected SATDs. Additionally, TEDIOUS and our CNN could complement existing smell and anti pattern detectors, such as DECOR \citep{moha2010decor}, to enhance their performance. Indeed, our research proved that TEDIOUS could outperform a smell detector of Long Methods and Long Parameter List.

Many improvement and work paths are already planned for the future. Both TEDIOUS and the CNN could benefit from configuration optimization. Only default parameters were used by the machine learners and extraction tools. More specifically, CNN results are still preliminary and performance metrics are still lacking. Further testing is required to better understand the real value of this approach. SATD comments were matched to methods with pattern matching, however, we retained only perfect matches, some comments did not match and others were very close to matching but were ultimately ignored. A revision of this process would be necessary to gather a more accurate picture of technical debts in systems.

As mentioned previously, our dataset is small and our approaches would benefit from a larger pool of examples to train on. More information could also be provided by adding other metrics or warnings to define methods. The same idea can be translated to the CNN by adding more source code. TEDIOUS only detects design debts and it would be interesting to see how well it performs for the prediction of all types of technical debts. The CNN already predicts all types of debts and performs better than TEDIOUS. We could also extend the application of our concept to other programing languages and domains. 

Finally, TEDIOUS is based on a Random Forests algorithm and we have a convolutional neural network which gives good prediction results. It would be interesting to test another popular machine learner used with Natural Language Processing, which is a \ac{RNN}.

%OPTIMIZATION
%MORE IN DEPTH UTILISATION AND ANALYSIS CNN
%TRY RNN
%REVIEW MATCHES OF SATD COMMENTS FOR MORE ACCURACY
%MORE SYSTEMS TO STUDY
%TRY NEW LANGUAGES/DOMAINS
%MORE INFORMATION TO TRAIN ON
%MORE FAMILIES OF TD



















